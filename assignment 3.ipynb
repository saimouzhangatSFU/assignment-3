{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 \n",
    "\n",
    "                                                                   https://github.com/saimouzhangatSFU/assignment-3\n",
    "                                                                    Saimou ZHANG 301306525\n",
    "                                                                    Amy Yang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saimouzhang/Documents/GitHub/assignment 3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the current working directory\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   pos  Stuning even for the non-gamer: This sound tra...\n",
       "1   pos  The best soundtrack ever to anything.: I'm rea...\n",
       "2   pos  Amazing!: This soundtrack is my favorite music...\n",
       "3   pos  Excellent Soundtrack: I truly like this soundt...\n",
       "4   pos  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Database source: https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('amazonreviews.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n"
     ]
    }
   ],
   "source": [
    "# take a look at the entire text for the first review\n",
    "print(df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the overall number of the reviews\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label     0\n",
       "review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there are any missing values. \n",
    "#If yes, remove them; otherwise, go ahead\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luckily, there are no missing values for either label or review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is a matrix \n",
    "X = df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is an one-dimensionary array\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train-test split, with the training size being 90%, while the test size being 10%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidVectorizer which combines the functionalities and processes of CountVectorizer and TfidfTransformer\n",
    "# For CountVectorizer, it serves the purpose of text preprocessing, tokenizing and removing stopwords.\n",
    "#                      This will build up a dictionary of features and transforms documents to feature vectors.\n",
    "# For TfidTransformer, it serves to divide the # of occurrences of each word by the total # of words in the document.\n",
    "#                      This is why it is called \"tf\" for Term Frequencies.\n",
    "#Besides, it would be helpful to downscale weights for words which occur in many documents in the corpus \n",
    "#,and which are thus less informative than those that occur only in a smaller portion of the corpus.\n",
    "# This downscaling is called \"tf–idf\" for “Term Frequency times Inverse Document Frequency”.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Linear Support Vector Classifier, which tends to perform better than naive bayes. \n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a compound classifier by building a pipeline, where both TfidfVectorizer and LinearSVC \n",
    "#are included to perform vectorization and classification. \n",
    "#This way, we don’t need to implement TfidfVectorizer and LinearSVC twice for the training set and the test set.\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('tfidf',TfidfVectorizer()),\n",
    "                     ('clf',LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the pipeline to fit our training data\n",
    "\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our raw test data, and form the prediction so that prediction value can be compared with the real y-test value below.\n",
    "\n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics to gauge the efficiency of the pipline/compound classifier include Confusion_matrix, classification_report \n",
    "#and accuracy_score. \n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[451  53]\n",
      " [ 70 426]]\n"
     ]
    }
   ],
   "source": [
    "#Compare the prediction values with the real y_test values by using confusion_matrix\n",
    "# precision is 451/(451+53) = 89.5, recall = 451/(451+70) = 86.6. These two values are fairly high, indeed.\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.89      0.88       504\n",
      "         pos       0.89      0.86      0.87       496\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.88      0.88      0.88      1000\n",
      "weighted avg       0.88      0.88      0.88      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare the prediction values with the real y_test values by using cliassification report \n",
    "# The compound classifer looks to work fairly well for BOTH negative and postive reveiws in terms of high precision,\n",
    "# recall, and f1-score values.\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877\n"
     ]
    }
   ],
   "source": [
    "# The overalll accuracy score is faily high\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The procedures of implementation\n",
    "\n",
    "\n",
    "1) Read the Amazon review dataset\n",
    "\n",
    "2) Check if there are any missing values for items; remove the rows/items where the missing values ever exist. \n",
    "\n",
    "3) Label the data as X and y, where “X” stands for a large matrix and “y” a one-dimensional array.\n",
    "\n",
    "4) Split the data into training set and test set by the proportion 9 to 1 to maximize the accuracy rate. Four subsets of data (X_train, X_test, y_train, y_test) are available now. \n",
    "\n",
    "5) Import TfidVectorizer which combines the processes and functionalities of CountVectorizer and TfidfTransformer. For CountVectorizer, it serves the purpose of text preprocessing, tokenizing and removing stop words. This builds up a dictionary of features (i.e., build a vocabulary, count the number of words) and transforms documents to be feature vectors. For TfidTransformer, it serves to divide the number of occurrences of each word by the total of words in the document. In addition to applying TfidTransformer for Term Frequencies, it is necessary to downscale weights for words which occur in many documents in the corpus, and which are thus less informative than those that occur only in a smaller portion of the corpus. This downscaling is called \"tf–idf\" for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "6) Import Linear Support Vector Classifier (to train a classifier)\n",
    "\n",
    "7) Create a compound classifier by building a pipeline, where both TfidfVectorizer and LinearSVC are included to perform vectorization and classification. This way, we don’t need to implement TfidfVectorizer and LinearSVC twice for the training set and the test set.\n",
    "\n",
    "8) Run the pipeline to fit ou training data\n",
    "\n",
    "9) Apply the pipeline from (7) to test the raw X_test data and form the prediction. \n",
    "\n",
    "10) Compare the prediction values with the real y_test values to examine the efficiency of the pipeline/compound classifier. Metrics to gauge the efficiency include Confusion_matrix, classification_report and accuracy_score. (Please see the code comments for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the area \n",
    "\n",
    "\n",
    "Sentiment analysis serves two main tasks: text classification, polarity of sentiment identification.\n",
    "\n",
    "Briefly, text classification means to input a text, and produces a numeric value that expresses the subjective content of a text, mostly in terms of its position on the spectrum of positivity-negativity. \n",
    "\n",
    "It has been widely used in movies reviews, customer reviews, market intelligence, and political propensity. \n",
    "\n",
    "Apart from extracting its degree of positivity-negativity, its polarity and strength, among other features, are also considered.\n",
    "\n",
    "Some challenges for sentiment analysis including identifying irony/sacastic content, negation, irrealis, out-of-topic sentence, appropriate world knowledge, among others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A survey of the methods used in that area\n",
    "\n",
    "1) Lexicon-based (bag of words) method in sentiment analysis:\n",
    "we associate words with rules (create a set of dictionaries) and apply these rules to test new data\n",
    "\n",
    "2)\tSupervised machine learning in sentiment analysis: \n",
    "Document/text & labels as the input, extract positive/negative features from the training data.\n",
    "Fit a classier to fit the training data.\n",
    "Some classier examples:\n",
    "•\tN-gram classifier where extract n-grams (unigrams, bigrams..) to create frequency list and then apply the list to the new/test data. The disadvantage of applying N-gram classifier may result in poor evaluation performance across domains. \n",
    "•\tNaive-Bayes Classfier, by applying the conditional probablity.\n",
    "•\tSVC (Support Vector classifier), exemplified in our coding and analysis .\n",
    "Utilize the classifier to make predictions, and compare with the test data.\n",
    "\n",
    "3) Deep learning in sentiment analysis:\n",
    "Supervised machine learning approach as well, with text & positive/negative labels available as the input.\n",
    "A hidden layer exists between the input layer and the output layer. \n",
    "However, little is known about what exact features are extracted in the hidden layer by the system.\n",
    "\n",
    "4) Unsupervised learning \n",
    "There are no labels associated with the data. \n",
    "By applying some algorithm, we could try to group the data, and identify some patterns.\n",
    "Specifically, the algorithms are applied to reduce the dimensions, and facilitate clustering.\n",
    "This method is helpful in that the topic clustering/modelling proves as another good way to identify\n",
    "the topics/groups of words that are common in positive or negative reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description of your own work within that area. \n",
    "\n",
    "\n",
    "Our project is mostly supervised machine learning, with some ideas, especially those relating to vectorization, bearing some relevance to the end of deep learning, though they may be systematically/ontologically different. \n",
    "\n",
    "Speicifically, in term of the classier selection, from the sklearn linear model packages, there are several models available to consider, including logistic regression model, naïve bayes model, support vector classifier (SVC) model. Some previous testing show that SVC model offers a better testing efficiency than the other two, and is thus chosen to be the classifier for our project. Together with vectorization, they are built into the pipeline. \n",
    "The testing performance is indeed satisfying, with the overall accuracy close to 90%. \n",
    "The advantages of applying SVC lie in the fact that 1) Term Frequency (tf) divides the number of occurrences of each word in a document by the total number of words in the document. 2)Term Frequency times Inverse Document Frequency (tf-idf)downscales weights for words that occur in many documents in the corpus and are thus less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "Supposedly, the cross-domain performance with linear SVC model would also be satisfying, which we can also further test out. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References at least to the tool itself and/or to papers describing the task and the tools.\n",
    "\n",
    "Amazon Reviews Dataset. Retrieved from https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
